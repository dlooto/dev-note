NOSQL--

===memcached, redis 比较

没有必要过多的关心性能，因为二者的性能都已经足够高了。由于Redis只使用单核，而Memcached可以使用多核，
所以在比较上，平均每一个核上Redis在存储小数据时比Memcached性能更高。而在100k以上的数据中，Memcached性能要
高于Redis，虽然Redis最近也在存储大数据的性能上进行优化，但是比起Memcached，还是稍有逊色。说了这么多，结论是，
无论你使用哪一个，每秒处理请求的次数都不会成为瓶颈。（比如瓶颈可能会在网卡）

如果要说内存使用效率，使用简单的key-value存储的话，Memcached的内存利用率更高，而如果Redis采用hash结构来做key-value存储，
由于其组合式的压缩，其内存利用率会高于Memcached。当然，这和你的应用场景和数据特性有关。

如果你对数据持久化和数据同步有所要求，那么推荐你选择Redis，因为这两个特性Memcached都不具备。即使你只是希望
在升级或者重启系统后缓存数据不会丢失，选择Redis也是明智的。

当然，最后还得说到你的具体应用需求。Redis相比Memcached来说，拥有更多的数据结构和并支持更丰富的数据操作，
通常在Memcached里，你需要将数据拿到客户端来进行类似的修改再set回去。这大大增加了网络IO的次数和数据体积。
在Redis中，这些复杂的操作通常和一般的GET/SET一样高效。所以，如果你需要缓存能够支持更复杂的结构和操作，那么Redis会是不错的选择。
来源：Is memcached a dinosaur in comparison to Redis?


======memcached最佳实践
　　但是也不太建议在任何情况下使用Memcached替代任何缓存：

　　1） 如果Value特别大，不太适合。因为在默认编译下Memcached只支持1M的Value（Key的限制到不是最大的问题）。其实从实践的角度来说也不建议把非常大的数据保存在Memcached中，因为有序列化反序列化的过程，别小看它消耗的CPU。说到这个就要提一下，我一直觉得Memcached适合面向输出的内容缓存，而不是面向处理的数据缓存，也就是不太适合把大块数据放进去拿出来处理之后再放进去，而是适合拿出来就直接给输出了，或是拿出来不需要处理直接用。

　　2） 如果不允许过期，不太适合。Memcached在默认情况下最大30天过期，而且在内存达到使用限制后它也会回收最少使用的数据。因此，如果我们要把它当作static变量的话就要考虑到这个问题，必须有重新初始化数据的过程。其实应该这么想，既然是缓存就是拿到了存起来，如果没有必定有一个重新获取重新缓存的过程，而不是想着它永远存在。


　　在使用Memcached的过程中当然也会有一些问题或者说最佳实践：

　　1） 清除部分数据的问题。Memcached只是一个Key/Value的池，一个公共汽车谁都可以上。我觉得对于类似的公共资源，如果用的人都按照自己的规则来的话很容易出现问题。因此，最好在Key值的规范上上使用类似命名空间的概念， 每一个用户都能很明确的知道某一块功能的Key的范围，或者说前缀。带来的好处是我们如果需要清空的话可以根据这个规范找到我们自己的一批Key然后再去清空，而不是清空所有的。当然有人是采用版本升级的概念，老的Key就让它过去吧，到时候自然会清空，这也是一种办法。不过Key有规范总是有好处的，在统计上也方便一点。

　　2） Value的组织问题。也就是说我们存的数据的粒度，比如要保存一个列表，是一个保存在一个键值还是统一保存为一个键值，这取决于业务。如果粒度很小的话最好是在获取的时候能批量获取，在保存的时候也能批量保存。对于跨网络的调用次数越少越好，可以想一下，如果一个页面需要输出100行数据，每一个数据都需要获取一次，一个页面进行上百次连接这个性能会不会成问题。

　　那么Memcached主要用在哪些功能上呢？

　　其实我觉得平时能想到在内存中做缓存的地方我们都可以考虑下是不是可以去适用分布式缓存，但是主要的用途还是用来在前端或中部挡一下读的需求来释放Web服务器App服务器以及DB的压力。


MongoDB的设计是要结合键值存储和关系型数据库的最好特性。键值存储，因为非常简单，所以速度极快而且相对容易伸缩。
关系型数据库较难伸缩，至少很难水平伸缩，但拥有富数据模型和强大的查询语言。如果MongoDB能介于两者之间，就能成为一款易伸缩、能存储丰富数据结构、提供复杂查询机制的数据库。


=====
存储媒体文件，可以考虑用 mongo gridfs 
那比如你一个 1u的服务器，能放 6T ，放满了的时候，你在加服务器怎么办？ 
又得去改代码，重新配置新服务器 
而用 gridfs ，只需要简单的配置 mongo 的分片就是，扩容完全不需要改代码得 
图片的元数据都直接用 gridfs 存储了 
从我目前了解的来看，数据增长不是很大的，或数据之意关系比较强的，比如帐户数据，可以用 MySQL；而其它弱关系的，比如图片文件和社交数据等，就用MongoDB 

===消息队列
比如新浪微博，你有1000万个粉丝，你post 一条微博，要通知所有的粉丝，你发新微博了。
你就可以把通知交给mq去排队发送。服务器速度快点就早点通知对方，慢点儿就晚点通知对方。
你就不需要在 app server 弄个线程来发噻.
反正这种通知又不需要太强的急时性.

消息队列:
    zeromq(C/C++开发),
    用 RabbitMQ , activemq 的公司可能比较多 
    amqp 是 MQ 的标准协议 

======张冬实现注册邮件发送的方式
把邮件信息全部放到一表中
用别外一个进程慢慢的去读，去发, 还不会存在丢失的问题.开进程是为了防止当前的进程堵或者其它原因，而且还可以多机器.
初始可以考虑开一个线程来处理.
该进程一直开启,无邮件可发时sleep几秒.
发完了, DB里的数据可不用删掉, 修改一个状态就好了, 删除其实挺浪费时间的, 如果实在没空间不够，定期删除也行.  

另外一场景: 我发了一条微博, 关注我的人都会收到一个提醒消息. 这个应该用消息队列更可行
    两种情况:推与拉
    关注者多就拉, 少就推

Dlooto2012-12-06 14:43:53	
    一个人的关注者, 一开始很少, 后来会增加, 初始用推, 后面再改成拉? 代码后期得改?
张冬2012-12-06 14:44:45	
    在准备的时候取一下关注者数嘛 

====redis




pip install python-memcached

====to cache template fragments
{% load cache %}
{% cache 500 sidebar %}
    .. sidebar ..
{% endcache %}

===cache view
urlpatterns = ('',
    (r'^foo/(\d{1,2})/$', cache_page(60 * 15)(my_view)),
)

from django.views.decorators.cache import cache_page

@cache_page(60 * 15, key_prefix="site1")
def my_view(request):
    ...

===cache API
>>> from django.core.cache import cache

如果有多个cache定义在CACHES中,可以用get_cache()通过key 检索cache object。
>>> from django.core.cache import get_cache
>>> cache = get_cache('alternate')

>>> cache.set('my_key', 'hello, world!', 30)   --超时时间
>>> cache.get('my_key')
'hello, world!'

>>> cache.set_many({'a': 1, 'b': 2, 'c': 3})
>>> cache.get_many(['a', 'b', 'c'])
{'a': 1, 'b': 2, 'c': 3}

>>> cache.delete('a')

>>> cache.delete_many(['a', 'b', 'c'])

clear() will remove everything from the cache, not just the keys set by your application.
cache.clear()


====redis install
download and extract package
cd redis-xxx/src
make install
cd utils
sudo ./install_server
--use all default sets

pip install -i http://192.168.1.114:8088/simple django-redis

====use redis
>>redis-cli -h 192.168.1.114 -p 6379    --具体用法  --help
    monitor  --实时查看redis数据存储情况 
    redis-cli -n 1   --指定db编号
    FLUSHALL         --刷新缓存
redis-benchmark   --可进行基准测试 



setex(key, time, value)：向库中添加string（名称为key，值为value）同时，设定过期时间time  --对String的操作

==cache usage:
from django.core.cache import cache
cache.set(key, value)
cache.set(key, value, timeout)
cache.get(key)

>>> cache.set_many({'a': 1, 'b': 2, 'c': 3})
>>> cache.get_many(['a', 'b', 'c'])
{'a': 1, 'b': 2, 'c': 3}

>>> cache.delete('a')

>>> cache.delete_many(['a', 'b', 'c'])

==cache key:
def make_key(key, key_prefix, version):
    return ':'.join([key_prefix, str(version), key])

====Django session
默认session被存储到DB.
By default, Django stores sessions in your database (using the model django.contrib.sessions.models.Session). Though this is convenient, in some setups it’s faster to store session data on your filesystem or in your cache.

    session存储在database-backed,由添加在INSTALLED_APPS中的'django.contrib.sessions'实现. 且 manage.py syncdb 安装数据库表

为了更好的性能, 可以使用cache-based session backend.

s = Session.objects.get(pk='xxxxxxxxxxxxx')
s.session_data
'KGRwMQpTJ19hdXRoX3VzZXJfaWQnCnAyCkkxCnMuMTExY2ZjODI2Yj...'
s.get_decoded()
{'user_id': 42}


cache key 命名约定: app:itemkey:id

==django session数据被保存时机
默认, session中新加入key/value对,或任何数据被修改或删除, session数据将被存储到DB, 如下:
    # Session is modified.
    request.session['foo'] = 'bar'

    # Session is modified.
    del request.session['foo']

    # Gotcha: Session is NOT modified, because this alters
    # request.session['foo'] instead of request.session.
    request.session['foo']['bar'] = 'baz'

    可以显示告诉session被修改了:
    request.session.modified = True

if set:
    SESSION_SAVE_EVERY_REQUEST = True
  Django will save the session to the database on every single request.

Note that the session cookie is only sent when a session has been created or modified. If SESSION_SAVE_EVERY_REQUEST is True, the session cookie will be sent on every request.
session被创建或修改时,session cookie才会被发送到客户端.

Similarly, the expires part of a session cookie is updated each time the session cookie is sent.  
cookie每被发送到客户端一次, cookie的过期时间被更新.
    Changed in Django 1.5: The session is not saved if the response’s status code is 500.

配置项: SESSION_EXPIRE_AT_BROWSER_CLOSE  (浏览器长度 vs. 持久化session)
    By default, SESSION_EXPIRE_AT_BROWSER_CLOSE is set to False, 浏览器关闭session不过期.

==清理session存储
sessoin数据会累积, 若用database backend, django_session数据库表将一直增长 .
当用户login时, django添加一条记录到django_session表, 当session数据修改,django将每次更新该记录.若用户手动登出,
django删除该记录.若用户未登出, 该记录将不会被删除.(用文件作为backend也一样)

django没有提供自动清理过期session数据的机制,所以, 需自定义定时job来完成过期数据的清理.
django提供了clearsessions命令可供使用, as a daily cron job.

用cache backend不存在这个问题, 因为cache会自动删除掉过期数据.

====redis使用策略
存放以下数据到cache:
    session数据
    全局共享数据
    每个用户当天需要频繁访问的数据, 可以在凌晨等时间提前放到redis
    排名数据

====Master and Slave 原理 
先讲主从原理吧：

当设置好slave服务器后，slave会建立和master的连接，然后发送sync命令。无论是第一次同步建立的连接还是连接断开后的重新连接（因此切记在高峰期设置slave），master都会启动一个后台进程，将数据库快照保存到文件中，同时master主进程会开始收集新的写命令并缓存起来。后台进程完成写文件后，master就发送文件给slave，slave将文件保存到磁盘上，然后加载到内存恢复数据库快照到slave上。接着master就会把缓存的命令转发给slave。而且后续master收到的写命令都会通过开始建立的连接发送给slave。从master到slave的同步数据的命令和从client发送的命令使用相同的协议格式。当master和slave的连接断开时slave可以自动重新建立连接。如果master同时收到多个 slave发来的同步连接命令，只会使用启动一个进程来写数据库镜像，然后发送给所有slave。 

设置方法：

主什么都不用管。



从：只需要修改redis.conf的slaveof

如:slaveof 192.168.20.192 6379 分别是ip和端口号。

注意事项：

1， 若主库挂了，不能直接开启主库程序。若直接开启主库程序将会冲掉从库的AOF文件，这样将导致数据丢失。

2， 一定要确保配置完全正确之后再重启。

3， 增加从库切














































